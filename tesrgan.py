# -*- coding: utf-8 -*-
"""Tesrgan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19_zu2MpPpbCMR-ssNHXsr9mdmzdiBhVW
"""

# Install required packages
!pip install torch torchvision pillow matplotlib kaggle

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms, datasets
from PIL import Image
import os
import matplotlib.pyplot as plt
from google.colab import drive

# Mount Google Drive for model saving
drive.mount('/content/drive')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Download COCO validation images (1GB total)
!wget -c http://images.cocodataset.org/zips/val2017.zip
!mkdir -p /content/coco
!unzip -q val2017.zip -d /content/coco/

print("COCO validation images extracted to /content/coco/val2017")

# Vision Transformer Components
class PatchEmbed(nn.Module):
    def __init__(self, img_size=32, patch_size=8, in_chans=64, embed_dim=512):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim=512, num_heads=8):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Linear(embed_dim * 4, embed_dim)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x

class ViTEnhancer(nn.Module):
    def __init__(self):
        super().__init__()
        self.blocks = nn.ModuleList([TransformerBlock() for _ in range(3)])

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

# Residual Dense Block
class ResidualDenseBlock_5C(nn.Module):
    def __init__(self, nf=64, gc=32):
        super().__init__()
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)
        self.conv3 = nn.Conv2d(nf + 2*gc, gc, 3, 1, 1)
        self.conv4 = nn.Conv2d(nf + 3*gc, gc, 3, 1, 1)
        self.conv5 = nn.Conv2d(nf + 4*gc, nf, 3, 1, 1)

    def forward(self, x):
        x1 = torch.relu(self.conv1(x))
        x2 = torch.relu(self.conv2(torch.cat((x, x1), 1)))
        x3 = torch.relu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = torch.relu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 + x

# RRDB-based Generator with Transformer Enhancement
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.scale = 4
        self.conv_first = nn.Conv2d(3, 64, 3, 1, 1)
        self.RRDB_trunk = nn.Sequential(*[ResidualDenseBlock_5C() for _ in range(23)])
        self.trunk_conv = nn.Conv2d(64, 64, 3, 1, 1)

        # Transformer Enhancement
        self.patch_embed = PatchEmbed()
        self.vit = ViTEnhancer()
        self.patch_unembed = nn.ConvTranspose2d(512, 64, kernel_size=8, stride=8)

        # Upsampling
        self.upconv1 = nn.Conv2d(64, 64 * 4, 3, 1, 1)
        self.upconv2 = nn.Conv2d(64, 64 * 4, 3, 1, 1)
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.conv_last = nn.Conv2d(64, 3, 3, 1, 1)

    def forward(self, x):
        fea = self.conv_first(x)
        trunk = self.trunk_conv(self.RRDB_trunk(fea)) + fea

        # Transformer Enhancement
        B, C, H, W = trunk.shape
        vit_input = self.patch_embed(trunk)
        vit_output = self.vit(vit_input)
        vit_output = vit_output.transpose(1, 2).view(B, 512, H//8, W//8)
        vit_output = self.patch_unembed(vit_output)
        trunk = trunk + vit_output

        # Upsampling
        out = self.pixel_shuffle(self.upconv1(trunk))
        out = self.pixel_shuffle(self.upconv2(out))
        out = self.conv_last(out)
        return torch.tanh(out)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(256, 1024, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(1024, 1, 1)
        )

    def forward(self, x):
        return self.net(x).view(-1)

# Define transforms
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load dataset using ImageFolder
train_dataset = datasets.ImageFolder(
    root='/content/coco',
    transform=transform
)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)

print(f"Dataset size: {len(train_dataset)}")

# Initialize models
generator = Generator().to(device)
discriminator = Discriminator().to(device)

# Loss functions
adversarial_loss = nn.BCEWithLogitsLoss()
content_loss = nn.MSELoss()

# Optimizers
opt_G = optim.Adam(generator.parameters(), lr=1e-4)
opt_D = optim.Adam(discriminator.parameters(), lr=1e-4)

# VGG for perceptual loss
vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True).features[:36].to(device).eval()
for param in vgg.parameters():
    param.requires_grad = False

def get_vgg_features(x):
    return vgg(x)

def visualize_results(lr, hr, fake):
    """Visualize sample images"""
    plt.figure(figsize=(15, 5))
    for i in range(3):
        # Low-res
        plt.subplot(3, 3, i*3+1)
        plt.imshow((lr[i].cpu().detach().permute(1,2,0).numpy() + 1)/2)
        plt.title("Input LR")
        plt.axis('off')

        # Fake
        plt.subplot(3, 3, i*3+2)
        plt.imshow((fake[i].cpu().detach().permute(1,2,0).numpy() + 1)/2)
        plt.title("Generated HR")
        plt.axis('off')

        # High-res
        plt.subplot(3, 3, i*3+3)
        plt.imshow((hr[i].cpu().detach().permute(1,2,0).numpy() + 1)/2)
        plt.title("Real HR")
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Training loop
epochs = 50
sample_interval = 50  # batches

for epoch in range(epochs):
    for i, (hr, _) in enumerate(train_loader):
        hr = hr.to(device)
        lr = transforms.functional.resize(hr, (32, 32))

        # Train Generator
        opt_G.zero_grad()
        fake = generator(lr)
        pred_fake = discriminator(fake)
        loss_GAN = adversarial_loss(pred_fake, torch.ones_like(pred_fake))

        # Perceptual Loss
        real_features = get_vgg_features(hr)
        fake_features = get_vgg_features(fake)
        loss_perceptual = content_loss(fake_features, real_features.detach())

        total_loss_G = loss_GAN + 0.1 * loss_perceptual
        total_loss_G.backward()
        opt_G.step()

        # Train Discriminator
        opt_D.zero_grad()
        pred_real = discriminator(hr)
        loss_real = adversarial_loss(pred_real, torch.ones_like(pred_real))
        pred_fake = discriminator(fake.detach())
        loss_fake = adversarial_loss(pred_fake, torch.zeros_like(pred_fake))
        loss_D = (loss_real + loss_fake) / 2
        loss_D.backward()
        opt_D.step()

        # Print progress
        if i % 10 == 0:
            print(f"[Epoch {epoch}/{epochs}] [Batch {i}/{len(train_loader)}] "
                  f"[D loss: {loss_D.item():.4f}] [G loss: {total_loss_G.item():.4f}]")

        # Save sample images
        batches_done = epoch * len(train_loader) + i
        if batches_done % sample_interval == 0:
            visualize_results(lr, hr, fake)

    # Save model checkpoints every epoch
    torch.save(generator.state_dict(), f"/content/drive/MyDrive/esrgan_gen_coco_{epoch}.pth")
    torch.save(discriminator.state_dict(), f"/content/drive/MyDrive/esrgan_disc_coco_{epoch}.pth")

# Save the generator model
model_path = "/content/drive/MyDrive/esrgan_transformer_model.pth"
torch.save(generator.state_dict(), model_path)
print(f"✅ Model saved to {model_path}")

# Save model class code to a file
model_code = """
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        # Your full model definition here
        # Vision Transformer Components
class PatchEmbed(nn.Module):
    def __init__(self, img_size=32, patch_size=8, in_chans=64, embed_dim=512):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim=512, num_heads=8):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Linear(embed_dim * 4, embed_dim)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x

class ViTEnhancer(nn.Module):
    def __init__(self):
        super().__init__()
        self.blocks = nn.ModuleList([TransformerBlock() for _ in range(3)])

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

# Residual Dense Block
class ResidualDenseBlock_5C(nn.Module):
    def __init__(self, nf=64, gc=32):
        super().__init__()
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)
        self.conv3 = nn.Conv2d(nf + 2*gc, gc, 3, 1, 1)
        self.conv4 = nn.Conv2d(nf + 3*gc, gc, 3, 1, 1)
        self.conv5 = nn.Conv2d(nf + 4*gc, nf, 3, 1, 1)

    def forward(self, x):
        x1 = torch.relu(self.conv1(x))
        x2 = torch.relu(self.conv2(torch.cat((x, x1), 1)))
        x3 = torch.relu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = torch.relu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 + x

# RRDB-based Generator with Transformer Enhancement
class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.scale = 4
        self.conv_first = nn.Conv2d(3, 64, 3, 1, 1)
        self.RRDB_trunk = nn.Sequential(*[ResidualDenseBlock_5C() for _ in range(23)])
        self.trunk_conv = nn.Conv2d(64, 64, 3, 1, 1)

        # Transformer Enhancement
        self.patch_embed = PatchEmbed()
        self.vit = ViTEnhancer()
        self.patch_unembed = nn.ConvTranspose2d(512, 64, kernel_size=8, stride=8)

        # Upsampling
        self.upconv1 = nn.Conv2d(64, 64 * 4, 3, 1, 1)
        self.upconv2 = nn.Conv2d(64, 64 * 4, 3, 1, 1)
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.conv_last = nn.Conv2d(64, 3, 3, 1, 1)

    def forward(self, x):
        fea = self.conv_first(x)
        trunk = self.trunk_conv(self.RRDB_trunk(fea)) + fea

        # Transformer Enhancement
        B, C, H, W = trunk.shape
        vit_input = self.patch_embed(trunk)
        vit_output = self.vit(vit_input)
        vit_output = vit_output.transpose(1, 2).view(B, 512, H//8, W//8)
        vit_output = self.patch_unembed(vit_output)
        trunk = trunk + vit_output

        # Upsampling
        out = self.pixel_shuffle(self.upconv1(trunk))
        out = self.pixel_shuffle(self.upconv2(out))
        out = self.conv_last(out)
        return torch.tanh(out)
"""

with open("/content/drive/MyDrive/esrgan_model_class.py", "w") as f:
    f.write(model_code)

# Import dependencies
import torch
import torch.nn as nn
from torchvision import transforms


# === Define All Components First ===

class PatchEmbed(nn.Module):
    def __init__(self, img_size=32, patch_size=8, in_chans=64, embed_dim=512):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.proj(x).flatten(2).transpose(1, 2)
        return x

class TransformerBlock(nn.Module):
    def __init__(self, embed_dim=512, num_heads=8):
        super().__init__()
        self.norm1 = nn.LayerNorm(embed_dim)
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 4),
            nn.GELU(),
            nn.Linear(embed_dim * 4, embed_dim)
        )

    def forward(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
        x = x + self.mlp(self.norm2(x))
        return x

class ViTEnhancer(nn.Module):
    def __init__(self):
        super().__init__()
        self.blocks = nn.ModuleList([TransformerBlock() for _ in range(3)])

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x

class ResidualDenseBlock_5C(nn.Module):
    def __init__(self, nf=64, gc=32):
        super().__init__()
        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)
        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)
        self.conv3 = nn.Conv2d(nf + 2*gc, gc, 3, 1, 1)
        self.conv4 = nn.Conv2d(nf + 3*gc, gc, 3, 1, 1)
        self.conv5 = nn.Conv2d(nf + 4*gc, nf, 3, 1, 1)

    def forward(self, x):
        x1 = torch.relu(self.conv1(x))
        x2 = torch.relu(self.conv2(torch.cat((x, x1), 1)))
        x3 = torch.relu(self.conv3(torch.cat((x, x1, x2), 1)))
        x4 = torch.relu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))
        return x5 + x

# === Then Define Generator ===

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.scale = 4
        self.conv_first = nn.Conv2d(3, 64, 3, 1, 1)
        self.RRDB_trunk = nn.Sequential(*[ResidualDenseBlock_5C() for _ in range(23)])
        self.trunk_conv = nn.Conv2d(64, 64, 3, 1, 1)

        # Transformer Enhancement
        self.patch_embed = PatchEmbed()
        self.vit = ViTEnhancer()
        self.patch_unembed = nn.ConvTranspose2d(512, 64, kernel_size=8, stride=8)

        # Upsampling
        self.upconv1 = nn.Conv2d(64, 64 * 4, 3, 1, 1)
        self.upconv2 = nn.Conv2d(64, 64 * 4, 3, 1, 1)
        self.pixel_shuffle = nn.PixelShuffle(2)
        self.conv_last = nn.Conv2d(64, 3, 3, 1, 1)

    def forward(self, x):
        fea = self.conv_first(x)
        trunk = self.trunk_conv(self.RRDB_trunk(fea)) + fea

        # Transformer Enhancement
        B, C, H, W = trunk.shape
        vit_input = self.patch_embed(trunk)
        vit_output = self.vit(vit_input)
        vit_output = vit_output.transpose(1, 2).view(B, 512, H//8, W//8)
        vit_output = self.patch_unembed(vit_output)
        trunk = trunk + vit_output

        # Upsampling
        out = self.pixel_shuffle(self.upconv1(trunk))
        out = self.pixel_shuffle(self.upconv2(out))
        out = self.conv_last(out)
        return torch.tanh(out)

    def forward(self, x):
        # Forward pass
      return x

# Load model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
generator = Generator().to(device)
generator.load_state_dict(torch.load("/content/drive/MyDrive/esrgan_transformer_model.pth"))
generator.eval()  # Set to evaluation mode

from google.colab import files

# Upload multiple files at once
uploaded = files.upload()

# Unzip the uploaded file
!unzip -q 'dataset_ver.zip' -d '/content/dataset_custom/'

import os
print(os.listdir('/content/dataset_custom/'))  # Verify contents

from torchvision import transforms, datasets
from torch.utils.data import DataLoader

# Define transforms
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize to match model input
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load dataset
custom_dataset = datasets.ImageFolder(
    root='/content/dataset_custom/',
    transform=transform
)

# Create DataLoader
custom_loader = DataLoader(custom_dataset, batch_size=4, shuffle=False)





transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Print dataset size
print(f"Total images: {len(custom_dataset)}")

# Get a batch of images
for images, _ in custom_loader:
    print("Batch shape:", images.shape)  # Should be (batch_size, 3, 128, 128)
    break

with torch.no_grad():
    for lr, _ in custom_loader:
        lr = lr.to(device)
        sr = generator(lr)  # Generate super-resolved images

# Load the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
generator = Generator().to(device)

# Load state_dict
generator.load_state_dict(torch.load("/content/drive/MyDrive/esrgan_transformer_model.pth"))
generator.eval()

from PIL import Image
import torchvision.transforms as transforms

# Define preprocessing
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load and preprocess image
def prepare_image(image_path):
    img = Image.open(image_path).convert("RGB")
    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dim
    return img, img_tensor

# Example
image_path = "/content/dataset_custom/dataset_ver/103.jpg"
original_img, lr_tensor = prepare_image(image_path)

# Generate super-resolved image
with torch.no_grad():
    sr_tensor = generator(lr_tensor)

# Denormalize and convert to PIL
def tensor_to_pil(tensor):
    tensor = tensor.cpu().squeeze(0)
    tensor = (tensor + 1) / 2  # Denormalize
    return transforms.ToPILImage()(tensor)

sr_pil = tensor_to_pil(sr_tensor)

# Display
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Low-Resolution")
plt.imshow(transforms.Resize((32, 32))(original_img))
plt.axis("off")

plt.subplot(1, 2, 2)
plt.title("Super-Resolved")
plt.imshow(sr_pil)
plt.axis("off")
plt.show()

# Create output directories
os.makedirs("/content/output/lr", exist_ok=True)
os.makedirs("/content/output/hr", exist_ok=True)
os.makedirs("/content/output/sr", exist_ok=True)

# Denormalize function
def denormalize(tensor):
    return (tensor * 0.5 + 0.5).clamp(0, 1)

# Convert tensor to PIL image
def tensor_to_pil(tensor):
    return transforms.ToPILImage()(denormalize(tensor).cpu())

# Run inference
with torch.no_grad():
    for batch_idx, (hr, _) in enumerate(custom_loader):
        hr = hr.to(device)
        lr = torch.nn.functional.interpolate(hr, scale_factor=0.25, mode='bicubic')  # Downscale

        # Generate super-resolved images
        fake = generator(lr)

        # Save images
        for i in range(fake.shape[0]):
            # Low-res (downscaled)
            lr_img = tensor_to_pil(lr[i])
            lr_img.save(f"/content/output/lr/lr_{batch_idx}_{i}.png")

            # High-res (original)
            hr_img = tensor_to_pil(hr[i])
            hr_img.save(f"/content/output/hr/hr_{batch_idx}_{i}.png")

            # Super-resolved
            sr_img = tensor_to_pil(fake[i])
            sr_img.save(f"/content/output/sr/sr_{batch_idx}_{i}.png")

print("✅ Generated images saved to /content/output/")